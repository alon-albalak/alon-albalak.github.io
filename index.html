<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Alon Albalak</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/custom.css" />
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<link rel="apple-touch-icon" sizes="180x180" href="images/favicon_io/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="images/favicon_black_background/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="images/favicon_black_background/favicon-16x16.png">
		<link rel="manifest" href="images/favicon_io/site.webmanifest">

		<!-- Google Analytics-->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-3DGYVRNC0N"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'G-3DGYVRNC0N');
		</script>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li>
								<img src="images/favicon_transparent_background/apple-touch-icon.png" alt="Logo" style="height: 50px; width: auto; vertical-align: middle;" />
							</li>
							<li><a href="#intro">Welcome</a></li>
							<li><a href="#about-me">About Me</a></li>
							<li><a href="#my-values">My Values</a></li>
							<li><a href="#selected-work">Selected Work</a></li>
							<!-- <li><a href="#two">Highlights</a></li> -->
							<li><a href="#contact">Contact</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Intro -->
				<section id="intro" class="wrapper style2 fullscreen fade-up">
					<div class="inner intro-flex">
						<div class="author-photo-container" style="flex-direction: column; display: flex; align-items: center;">
							<img src="images/alon.jpg" alt="Alon Albalak" class="author-photo" />
							<div class="author-socials" style="margin-top:1em; text-align:center; width:100%;">
								<a href="https://x.com/AlbalakAlon" class="icon brands fa-twitter" target="_blank" rel="noopener" title="Twitter" style="margin:0 0.3em; font-size:2.2em;"><span class="label">Twitter</span></a>
								<a href="https://scholar.google.com/citations?user=F6J_7d8AAAAJ" class="ai ai-google-scholar" target="_blank" rel="noopener" title="Google Scholar" style="margin:0 0.3em; font-size:2.5em;"><span class="label">Google Scholar</span></a>
								<a href="https://www.linkedin.com/in/alonalbalak/" class="icon brands fa-linkedin-in" target="_blank" rel="noopener" title="LinkedIn" style="margin:0 0.3em; font-size:2em;"><span class="label">LinkedIn</span></a>
								<a href="https://github.com/alon-albalak" class="icon brands fa-github" target="_blank" rel="noopener" title="GitHub" style="margin:0 0.3em; font-size:2em;"><span class="label">GitHub</span></a>
								<a href="mailto:alonalbalak@gmail.com" class="icon solid fa-envelope" title="Email" style="margin:0 0.3em; font-size:2em;"><span class="label">Email</span></a>
							</div>
						</div>
						<div class="intro-text">
							<h1>Hi, I'm <strong style="color: #fff;">Alon</strong>!</h1>
							<p>I'm a research scientist on the Open-Endedness team at <a href="https://www.lila.ai/">Lila Sciences</a>, where I research AI that doesn’t just solve problems, but creatively explores new scientific frontiers.</p>
							<ul class="actions">
								<li><a href="#about-me" class="button scrolly">Learn more</a></li>
							</ul>
						</div>
					</div>
				</section>

				<!-- About me -->
				<section id="about-me" class="wrapper alt fade-up">
					<div class="inner">
						<h2>About Me</h2>
						<p>
							The primary focus of my research has been on the intersection of machine learning and data (data-centric AI).<br>
							<strong>Currently, I'm applying my background to improving automated scientific discovery by building systems that collaborate with and empower human researchers.</strong>
						</p>
						<p>
							I received my Ph.D. in the <a href="http://nlp.cs.ucsb.edu/">NLP Group</a> at the University of California, Santa Barbara, advised by professors <a href="https://sites.cs.ucsb.edu/~william/">William Yang Wang</a> and <a href="https://sites.cs.ucsb.edu/~xyan/">Xifeng Yan</a>.
							During the first year of my Ph.D. I was gratefully supported by an NSF IGERT Fellowship. While pursuing my Ph.D. I took a year off from research to work at a financial technology startup, <a href="https://thetalake.com/">Theta Lake</a>.
							Prior to my Ph.D. I received my B.S. in mathematics at Wayne State University, with research advised by <a href="https://scholar.google.com/citations?user=nlZ0o_4AAAAJ">Gang George Yin</a>.
						</p>
						<ul class="actions">
							<li><a href="images/albalak_resume.pdf" class="button primary" target="_blank" rel="noopener">View Resume (PDF)</a></li>
						</ul>
					</div>
				</section>

				<!-- My Values -->
				<section id="my-values" class="wrapper style1 fade-up">
					<div class="inner">
						<h2>My Values</h2>
						<div class="features">
							<section>
								<span class="icon solid major fa-trophy"></span>
								<h3>Challenge</h3>
								<p>To keep challenging myself to <strong>grow, learn, and improve</strong> in knowledge, skills, character, and life experience.</p>
							</section>
							<section>
								<span class="icon solid major fa-users"></span>
								<h3>Connect</h3>
								<p>
									To engage fully in whatever I am doing and be <em>fully present</em> with others.<br>
									To be <strong>supportive, helpful, and encouraging</strong> to myself and others.
								</p>
							</section>
							<section>
								<span class="icon solid major fa-lightbulb"></span>
								<h3>Curiosity</h3>
								<p>
									To be curious, open-minded, and interested; to <strong>explore and discover</strong>.<br>
									To think things through, see things from others’ points of view and <em>weigh evidence fairly</em>.
								</p>
							</section>
						</div>
					</div>
				</section>


			   <!-- Selected Work -->
   <section id="selected-work" class="wrapper style3 spotlights">
	<div class="inner">
	<header class="major" style="text-align:left; margin-bottom:2em;">
	   <h2>Selected Work</h2>
	<p class="section-subtitle">
		A selection of recent research papers and projects. Full list on 
		<a href="https://scholar.google.com/citations?user=F6J_7d8AAAAJ" target="_blank" rel="noopener">Google Scholar</a>
	</p>
	 </header>
	 </div> 
				 <section>
				   <a href="https://arxiv.org/abs/2502.17387" class="image" target="_blank" rel="noopener"><img src="images/bigmath.png" alt="Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models" /></a>
				   <div class="content">
					 <div class="inner">
					   <h3>Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement Learning in Language Models</h3>
					   <p><strong>Big-Math is a dataset of over 250,000 high-quality math questions designed for reinforcement learning</strong> (RL) research. Unlike existing datasets that often prioritize either quality or quantity, Big-Math bridges this gap by offering a large collection of rigorously curated problems with verifiable answers. It includes a diverse range of problem domains and difficulty levels, making it suitable for models of varying capabilities. Additionally, Big-Math-Reformulated introduces <strong>47,000 multiple-choice questions</strong> reformulated as open-ended problems. This dataset provides a robust foundation for advancing reasoning in large language models (LLMs).</p>
					   <ul class="actions">
						 <li><a href="https://arxiv.org/abs/2502.17387" class="button primary" target="_blank" rel="noopener">Link to paper</a></li>
					   </ul>
					 </div>
				   </div>
				 </section>
				 <section>
				   <a href="https://arxiv.org/abs/2410.12832" class="image" target="_blank" rel="noopener"><img src="images/genrm.png" alt="Generative reward models" /></a>
				   <div class="content">
					 <div class="inner">
					   <h3>Generative reward models</h3>
					   <p>This work introduces <strong>GenRM</strong>, an iterative algorithm that combines Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) to improve the quality of synthetic preference labels. While synthetic preferences generated by LLMs have been shown to misalign with human judgments, GenRM trains an LLM on self-generated reasoning traces, aligning synthetic preferences with human preferences. The approach outperforms existing models, achieving comparable accuracy to Bradley-Terry reward models on in-distribution tasks, while <strong>significantly surpassing them on out-of-distribution tasks</strong>. The results highlight the potential of this hybrid approach for enhancing the effectiveness of synthetic feedback in RL.</p>
					   <ul class="actions">
						 <li><a href="https://arxiv.org/abs/2410.12832" class="button primary" target="_blank" rel="noopener">Link to paper</a></li>
					   </ul>
					 </div>
				   </div>
				 </section>
				 <section>
				   <a href="https://arxiv.org/abs/2402.16827" class="image" target="_blank" rel="noopener"><img src="images/data_selection_survey.png" alt="A Survey on Data Selection for Language Models" /></a>
				   <div class="content">
					 <div class="inner">
					   <h3>A Survey on Data Selection for Language Models</h3>
					   <p>This work provides a comprehensive review of data selection methods for training large language models (LLMs), an area crucial for improving training efficiency and reducing environmental and financial costs. It presents a taxonomy of existing approaches, highlighting the challenges and opportunities in selecting high-quality data for model pre-training and post-training. By mapping out the current landscape and identifying gaps in the literature, this review aims to accelerate progress in the field, offering valuable insights for both new and experienced researchers. The paper also proposes promising directions for future research to address these gaps.</p>
					   <ul class="actions">
						 <li><a href="https://arxiv.org/abs/2402.16827" class="button primary" target="_blank" rel="noopener">Link to paper</a></li>
					   </ul>
					 </div>
				   </div>
				 </section>
				 <section>
				   <a href="https://arxiv.org/abs/2312.02406" class="image" target="_blank" rel="noopener"><img src="images/odm.png" alt="Efficient Online Data Mixing For Language Model Pre-Training" /></a>
				   <div class="content">
					 <div class="inner">
					   <h3>Efficient Online Data Mixing For Language Model Pre-Training</h3>
					   <p>This work introduces Online Data Mixing (<strong>ODM</strong>), an efficient algorithm that combines data selection and data mixing to optimize the pretraining process of large language models. Unlike traditional methods that use fixed mixing proportions, ODM adapts these proportions dynamically during training, improving computational efficiency. The approach, based on multi-armed bandit algorithms, <strong>reduces training iterations by 19%</strong>, achieves superior performance on the 5-shot MMLU benchmark (<strong>1.9% relative accuracy gain</strong>), and adds minimal extra time to pretraining. This method addresses the inefficiencies of existing data selection approaches, making it a promising advancement for large-scale model training.</p>
					   <ul class="actions">
						 <li><a href="https://arxiv.org/abs/2312.02406" class="button primary" target="_blank" rel="noopener">Link to paper</a></li>
					   </ul>
					 </div>
				   </div>
				 </section>
				 <section>
				   <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/4e3c5399729e06d2f0c22d04416904ab-Abstract-Conference.html" class="image" target="_blank" rel="noopener"><img src="images/flad.png" alt="Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data" /></a>
				   <div class="content">
					 <div class="inner">
					   <h3>Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data</h3>
					   <p>In this work, we study Few-shot Learning with Auxiliary Data (<strong>FLAD</strong>), a paradigm that improves generalization in few-shot learning by incorporating auxiliary datasets. Unlike previous methods, which scale poorly with the number of auxiliary datasets, the proposed algorithms, EXP3-FLAD and UCB1-FLAD, overcome this limitation by achieving computational complexity independent of dataset size. Motivated by multi-armed bandit algorithms, we combine exploration and exploitation into algorithms that outperform existing FLAD methods by 4% and <strong>enable 3-billion-parameter language models to surpass the performance of GPT-3 (175 billion parameters)</strong>. This work suggests that more efficient mixing strategies for FLAD can significantly enhance few-shot learning.</p>
					   <ul class="actions">
						 <li><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/4e3c5399729e06d2f0c22d04416904ab-Abstract-Conference.html" class="button primary" target="_blank" rel="noopener">Link to paper</a></li>
					   </ul>
					 </div>
				   </div>
				 </section>
				 <section>
				   <a href="https://arxiv.org/abs/2205.06262" class="image" target="_blank" rel="noopener"><img src="images/feta.png" alt="FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue" /></a>
				   <div class="content">
					 <div class="inner">
					   <h3>FETA: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue</h3>
					   <p>This work introduces <strong>FETA</strong>, a benchmark for few-sample task transfer in open-domain dialogue, aiming to reduce the need for labeled data in fine-tuning language models. FETA includes two conversation datasets with 10 and 7 tasks, enabling the study of intra-dataset task transfer—transferring knowledge between related tasks without domain adaptation. The paper explores task transfer across <em>132 source-target pairs</em> using three popular language models and learning algorithms, providing insights into model-specific performance trends. Key findings highlight that span extraction and multiple-choice tasks benefit most from task transfer. Beyond task transfer, FETA offers a resource for future research on model efficiency, pre-training datasets, and learning settings like continual and multitask learning.</p>
					   <ul class="actions">
						 <li><a href="https://arxiv.org/abs/2205.06262" class="button primary" target="_blank" rel="noopener">Link to paper</a></li>
					   </ul>
					 </div>
				   </div>
				 </section>
				 <section>
				   <a href="https://arxiv.org/abs/2109.05126" class="image" target="_blank" rel="noopener"><img src="images/d-rex.png" alt="D-REX: Dialogue Relation Extraction with Explanations" /></a>
				   <div class="content">
					 <div class="inner">
					   <h3>D-REX: Dialogue Relation Extraction with Explanations</h3>
					   <p>This work introduces <strong>D-REX</strong>, a model-agnostic, policy-guided semi-supervised framework for cross-sentence relation extraction in multi-party conversations, with a focus on explainability. D-REX frames relation extraction as a re-ranking task and provides relation- and entity-specific explanations as an intermediate step. The method <strong>outperforms existing models by 13.5% in F1 score</strong> on a dialogue relation extraction dataset and is <strong>preferred by human annotators 90% of the time</strong> over a strong BERT-based model. D-REX offers a simple yet effective approach, advancing both the explainability and accuracy of relation extraction in long-form dialogues.</p>
					   <ul class="actions">
						 <li><a href="https://arxiv.org/abs/2109.05126" class="button primary" target="_blank" rel="noopener">Link to paper</a></li>
					   </ul>
					 </div>
				   </div>
				 </section>
			   </section>

				<!-- Two -->
				<!--
					<section id="two" class="wrapper style3 fade-up">
						<div class="inner">
							<h2>What we do</h2>
							<p>Phasellus convallis elit id ullamcorper pulvinar. Duis aliquam turpis mauris, eu ultricies erat malesuada quis. Aliquam dapibus, lacus eget hendrerit bibendum, urna est aliquam sem, sit amet imperdiet est velit quis lorem.</p>
							<div class="features">
								<section>
									<span class="icon solid major fa-code"></span>
									<h3>Lorem ipsum amet</h3>
									<p>Phasellus convallis elit id ullam corper amet et pulvinar. Duis aliquam turpis mauris, sed ultricies erat dapibus.</p>
								</section>
								<section>
									<span class="icon solid major fa-lock"></span>
									<h3>Aliquam sed nullam</h3>
									<p>Phasellus convallis elit id ullam corper amet et pulvinar. Duis aliquam turpis mauris, sed ultricies erat dapibus.</p>
								</section>
								<section>
									<span class="icon solid major fa-cog"></span>
									<h3>Sed erat ullam corper</h3>
									<p>Phasellus convallis elit id ullam corper amet et pulvinar. Duis aliquam turpis mauris, sed ultricies erat dapibus.</p>
								</section>
								<section>
									<span class="icon solid major fa-desktop"></span>
									<h3>Veroeros quis lorem</h3>
									<p>Phasellus convallis elit id ullam corper amet et pulvinar. Duis aliquam turpis mauris, sed ultricies erat dapibus.</p>
								</section>
								<section>
									<span class="icon solid major fa-link"></span>
									<h3>Urna quis bibendum</h3>
									<p>Phasellus convallis elit id ullam corper amet et pulvinar. Duis aliquam turpis mauris, sed ultricies erat dapibus.</p>
								</section>
								<section>
									<span class="icon major fa-gem"></span>
									<h3>Aliquam urna dapibus</h3>
									<p>Phasellus convallis elit id ullam corper amet et pulvinar. Duis aliquam turpis mauris, sed ultricies erat dapibus.</p>
								</section>
							</div>
							<ul class="actions">
								<li><a href="generic.html" class="button">Learn more</a></li>
							</ul>
						</div>
					</section>
				-->

			   <!-- Contact -->
				<section id="contact" class="wrapper style2">
				<div class="inner" style="text-align:center;">
					<h2>Contact Me</h2>
					<p>Email me at "alonalbalak at gmail dot com"</p>
				</div>
				</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper style1">
				<div class="inner">
					<ul class="menu">
						<li>&copy; (2023-2025) Alon Albalak. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
					<img src="images/favicon_transparent_background/apple-touch-icon.png" alt="Logo" style="width: 50px; height: 50px; display: block; margin: 1em auto 0 auto;" />
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>