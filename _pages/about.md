---
permalink: /
title: "About Me - Alon Albalak"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a fifth year Ph.D. candidate in the [NLP Group](http://nlp.cs.ucsb.edu/) at the University of California, Santa Barbara. I am gratefully advised by professors [William Yang Wang](https://sites.cs.ucsb.edu/~william/) and [Xifeng Yan](https://sites.cs.ucsb.edu/~xyan/). During the first year of my Ph.D. I was gratefully supported by an [NSF IGERT Fellowship](http://www.igert.org/). While pursuing my Ph.D. I took a year off from research to work at a financial technology startup, [Theta Lake](https://thetalake.com/). Prior to UCSB I received my B.S. in mathematics at [Wayne State University](https://clas.wayne.edu/math).

My research interests include a broad range of topics within machine learning and natural language processing.
However the *main focus* of my research is to find new methods of **efficiently using limited amounts of data**. In my path to improving data efficiency I have utilized:
1. Multitask learning ([1](https://arxiv.org/abs/2302.00674),[2](https://neurips2022-enlsp.github.io/papers/paper_50.pdf))
2. Transfer learning ([1](https://aclanthology.org/2022.emnlp-main.751/),[2](https://aclanthology.org/2022.nlp4convai-1.4/),[3](https://assets.amazon.science/80/f0/ad9a999f4562b6e80186a5df00e6/making-something-out-of-nothing-building-robust-task-oriented-dialogue-systems-from-scratch.pdf)),
3. Data augmentation ([1](https://assets.amazon.science/80/f0/ad9a999f4562b6e80186a5df00e6/making-something-out-of-nothing-building-robust-task-oriented-dialogue-systems-from-scratch.pdf),[2](https://arxiv.org/abs/2201.11153)),
4. Neuro-symbolic methods ([1](https://arxiv.org/abs/2205.14268), [2](https://openreview.net/pdf?id=8ZIJa8Z__5L), [3](https://arxiv.org/abs/2207.07238)).

<hr>

## \*\* NEWS \*\*

### \[02/23\] New preprint on "Improving Few-shot Generalization by Exploring and Exploiting Auxiliary Data" is out! [Preprint](https://arxiv.org/abs/2302.00674)

### \[01/23\] The Transfer Learning for NLP Workshop (TL4NLP) workshop is available to watch!
TL4NLP explored insights and advances on transfer learning, including insightful talks from our guest speakers and hot takes from our debaters.<br>
Check out the amazing lineup of speakers, topics, and more at [tl4nlp.githb.io](https://tl4nlp.github.io).<br>
[Watch the full workshop here](https://neurips.cc/virtual/2022/workshop/50006).

<!--
Starting Timestamps:
Jonas Pfeiffer - 36:55
Graham Neubig - 1:30:45 (describes my current project at 2:10:50-2:11:40)
Percy Liang & Ananya Kumar - 2:13:33
Sara Hooker and Kyunghyun Cho - 3:13:19
David Adelani - 5:42:45
Mike Lewis - 6:25:46
-->

### \[10/22\] My paper on benchmarking task transfer will be at EMNLP '22: [FETA](https://arxiv.org/abs/2205.06262)
FETA is the largest NLP benchmark for intra-dataset task transfer, where task transfer is isolated from domain shift.<br>
Check out the [paper](https://arxiv.org/abs/2205.06262), and our [github repo](https://github.com/alon-albalak/TLiDB).

### \[10/22\] My work on data effiency for small(-ish) language models will be presented an ENLSP '22: [Paper](https://neurips2022-enlsp.github.io/papers/paper_50.pdf)

### \[05/22\] I will join Meta as a Research Science Intern for summer 2022!

### \[05/22\] The UCSB [GauchoBot](https://www.amazon.science/alexa-prize/teams/university-of-california-santa-barbara-team-gauchobot) has advanced to the finals of the Alexa Prize Taskbot Challenge!

### \[04/22\] [D-REX](https://aclanthology.org/2022.nlp4convai-1.4/) accepted to ConvAI workshop, co-located with ACL 2022
